{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89eb636",
   "metadata": {},
   "source": [
    "Notebook for experimenting with what's implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0074443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddca0a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "de_en_tokenizer = Tokenizer.from_file(\"tokenizers/de_en_tokenizer.json\")\n",
    "\n",
    "de_en_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63fe37d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fe8bf452154aa08963df4a3f95cb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartekryba/Library/Caches/pypoetry/virtualenvs/dnlp2025-V-ztQv5h-py3.12/lib/python3.12/site-packages/torch/utils/data/sampler.py:68: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from datasets import Dataset\n",
    "import dnlp2025.dataset  # your local module\n",
    "importlib.reload(dnlp2025)\n",
    "\n",
    "def get_mock_split_small():\n",
    "    return Dataset.from_list([\n",
    "        { \"translation\": { \"de\": \"Guten morgen.\", \"en\": \"Good morning.\" }},\n",
    "        { \"translation\": { \"de\": \"Guten tag.\", \"en\": \"Good day.\" }},\n",
    "        { \"translation\": { \"de\": \"Wilkommen.\", \"en\": \"Welcome.\" }},\n",
    "        { \"translation\": { \"de\": \"Wie geht es Ihnen?\", \"en\": \"How are you?\" }},\n",
    "        { \"translation\": { \"de\": \"Das ist ein Buch.\", \"en\": \"That is a book.\" }},\n",
    "        { \"translation\": { \"de\": \"Ich denke, dass ich einen Kaffee brauche.\", \"en\": \"I think I need a coffee.\" }},\n",
    "        { \"translation\": { \"de\": \"Ich bin ein bisschen m dde.\", \"en\": \"I am a bit tired.\" }}\n",
    "    ])\n",
    "\n",
    "mock_split = Dataset.from_list(get_mock_split_small())\n",
    "\n",
    "de_en_train_dataloader = dnlp2025.dataset.create_dataloader(\n",
    "    mock_split,\n",
    "    \"test\", # If set to \"train\", then the last batch will be removed\n",
    "    de_en_tokenizer,\n",
    "    max_tokens_per_batch=20,\n",
    "    shuffle=False # Shuffling disabled to see how the batches are created\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8d4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token ids: Padding - 0, EOS - 1, BOS - 2\n",
      "\n",
      "Batch:  1\n",
      "Encoder input ids: \n",
      "tensor([[    1, 14070,    16,     2,     0,     0],\n",
      "        [    1, 12492, 10038,    16,     2,     0],\n",
      "        [    1, 12492,  4328,    16,     2,     0],\n",
      "        [    1,  7815,  3862,  3974,    33,     2]])\n",
      "\n",
      "Decoder input ids (should strip EOS token): \n",
      "tensor([[    1, 12905,  4391,    16,     0,     0],\n",
      "        [    1, 34138,  9304,    16,     0,     0],\n",
      "        [    1, 34138,  6169,    16,     0,     0],\n",
      "        [    1,  5118,  5255,  3769,  4755,    33]])\n",
      "\n",
      "Label ids (should strip BOS token): \n",
      "tensor([[12905,  4391,    16,     2,     0,     0],\n",
      "        [34138,  9304,    16,     2,     0,     0],\n",
      "        [34138,  6169,    16,     2,     0,     0],\n",
      "        [ 5118,  5255,  3769,  4755,    33,     2]])\n",
      "\n",
      "Batch:  2\n",
      "Encoder input ids: \n",
      "tensor([[    1,  5336,  3772,    67,  5689,    16,     2,     0],\n",
      "        [    1,    43,  3811,    67,  5760, 28026,    16,     2]])\n",
      "\n",
      "Decoder input ids (should strip EOS token): \n",
      "tensor([[    1,  4151,  3831,  3791,  7057,    16,     0,     0,     0],\n",
      "        [    1,  4191,  4681,  3791, 21375,    79,    70,  3795,    16]])\n",
      "\n",
      "Label ids (should strip BOS token): \n",
      "tensor([[ 4151,  3831,  3791,  7057,    16,     2,     0,     0,     0],\n",
      "        [ 4191,  4681,  3791, 21375,    79,    70,  3795,    16,     2]])\n",
      "\n",
      "Batch:  3\n",
      "Encoder input ids: \n",
      "tensor([[    1,    43,  5351,    43,  4413,    67, 11480,    16,     2]])\n",
      "\n",
      "Decoder input ids (should strip EOS token): \n",
      "tensor([[    1,  4191,  8001,    14,  3963,  3783,  4043, 15463, 35602,    16]])\n",
      "\n",
      "Label ids (should strip BOS token): \n",
      "tensor([[ 4191,  8001,    14,  3963,  3783,  4043, 15463, 35602,    16,     2]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Special token ids: Padding - 0, EOS - 1, BOS - 2\")\n",
    "\n",
    "batch_num = 0\n",
    "for batch in de_en_train_dataloader:\n",
    "    batch_num += 1\n",
    "\n",
    "    print(\"\\nBatch: \", batch_num)\n",
    "    print(\"Encoder input ids: \")\n",
    "    print(batch['encoder_input_ids'])\n",
    "\n",
    "    print(\"\\nDecoder input ids (should strip EOS token): \")\n",
    "    print(batch['decoder_input_ids'])\n",
    "\n",
    "    print(\"\\nLabel ids (should strip BOS token): \")\n",
    "    print(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f226d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dnlp2025.download_datasets import download_wmt14_de_en\n",
    "\n",
    "de_en_dataset = download_wmt14_de_en()\n",
    "small_dataset = de_en_dataset['train'].train_test_split(train_size=0.1, seed=42)['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d892d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13071da07990436bb9b85a396692c65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset (num_proc=10):   0%|          | 0/450878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartekryba/Library/Caches/pypoetry/virtualenvs/dnlp2025-V-ztQv5h-py3.12/lib/python3.12/site-packages/torch/utils/data/sampler.py:68: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "de_en_full_dataloader = dnlp2025.dataset.create_dataloader(\n",
    "    small_dataset,\n",
    "    \"train\",\n",
    "    de_en_tokenizer,\n",
    "    max_tokens_per_batch=25000,\n",
    "    shuffle=True,\n",
    "    num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0172a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_en_full_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86115086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder_input_ids': tensor([[    1,  4505,  4006,  ...,  7538,    16,     2],\n",
      "        [    1,    41,  7410,  ..., 34734,    16,     2],\n",
      "        [    1,  3869,  3971,  ...,  5665,    16,     2],\n",
      "        ...,\n",
      "        [    1,  4179,  3772,  ...,  3998,    16,     2],\n",
      "        [    1, 13791,  3790,  ..., 10277,    16,     2],\n",
      "        [    1,  8821,  3801,  ...,  6806,    16,     2]]), 'decoder_input_ids': tensor([[    1,  4527, 16724,  ...,     0,     0,     0],\n",
      "        [    1,  4151,    41,  ...,     0,     0,     0],\n",
      "        [    1,  3869,  5312,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1, 18205,  4031,  ...,     0,     0,     0],\n",
      "        [    1,  3972, 18404,  ...,     0,     0,     0],\n",
      "        [    1, 10236, 15128,  ...,     0,     0,     0]]), 'labels': tensor([[ 4527, 16724,  4034,  ...,     0,     0,     0],\n",
      "        [ 4151,    41,  7410,  ...,     0,     0,     0],\n",
      "        [ 3869,  5312, 15210,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [18205,  4031,  4354,  ...,     0,     0,     0],\n",
      "        [ 3972, 18404,  3878,  ...,     0,     0,     0],\n",
      "        [10236, 15128,  4081,  ...,     0,     0,     0]]), 'source_key_padding_mask': tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]]), 'target_key_padding_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]]), 'target_mask': tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in de_en_full_dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
